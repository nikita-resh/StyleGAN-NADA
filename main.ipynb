{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikita-resh/StyleGAN-NADA/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1IrCWriCkn_"
      },
      "outputs": [],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install Ninja\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "import clip\n",
        "import gc\n",
        "import time\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "!git clone https://github.com/rosinality/stylegan2-pytorch.git\n",
        "%cd stylegan2-pytorch\n",
        "from model import Generator\n",
        "\n",
        "\n",
        "file_path = \"/content/stylegan2-pytorch/lpips/__init__.py\"\n",
        "# Read file content\n",
        "with open(file_path, \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "# Write updated content\n",
        "with open(file_path, \"w\") as file:\n",
        "    file.writelines(\"from skimage.metrics import structural_similarity\\n\" if \"compare_ssim\" in line else line for line in lines)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# download wights for stylegan2-ffhq\n",
        "gdown.download(\"https://drive.google.com/uc?id=1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT\", \"stylegan2-ffhq-config-f.pt\", quiet=False)\n",
        "\n",
        "%cd ..\n",
        "!git clone https://github.com/omertov/encoder4editing.git\n",
        "%cd encoder4editing\n",
        "\n",
        "from models.psp import pSp\n",
        "from argparse import Namespace\n",
        "\n",
        "# download wights for e4e-ffhq\n",
        "gdown.download(\"https://drive.google.com/uc?id=1cUv_reLE6k3604or78EranS7XzuVMWeO\", \"e4e_ffhq_encode.pt\", quiet=False)\n",
        "\n",
        "from utils.common import tensor2im"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DswhREmsUm2"
      },
      "source": [
        "### CLIP Loss\n",
        "\n",
        "We want the model to generate images that align with the properties described in the text prompt. To achieve this, we use CLIP loss to encourage the model to generate images that are closer to the text description in the CLIP embedding space.\n",
        "\n",
        "The CLIP loss is computed as follows:\n",
        "\n",
        "$$\n",
        "D_{\\text{CLIP}} = 1 - \\frac{\\langle E_T(G(w)) \\cdot E_T(t) \\rangle }{\\|E_T(G(w))\\| \\cdot \\|E_T(t)\\|}\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "\n",
        "- $E_T$: Text and image encoder function (e.g., CLIP's shared encoder for both text and image).\n",
        "- $t$: Text description (prompt).\n",
        "- $G(w)$: Image generated by the generator using the latent vector $w$.\n",
        "- $D_{\\text{CLIP}}$: Cosine distance between the text embedding and the image embedding in the CLIP space.\n",
        "\n",
        "**Intuition:** The CLIP loss encourages the generated image $G(w)$ to be semantically aligned with the provided text prompt $t$. If the image and text embeddings are close in the CLIP space, the loss will be small, helping the model generate images that reflect the described properties.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sc72kwOkJRN"
      },
      "outputs": [],
      "source": [
        "class CLIPLoss(torch.nn.Module):\n",
        "    def __init__(self, stylegan_size=1024):\n",
        "        super(CLIPLoss, self).__init__()\n",
        "\n",
        "        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "        # clip only supports images 224*224.\n",
        "        # we need to resize image with min information loss possible. therefore we firstly apply upsamling, and then pooling.\n",
        "        # scale_factor=7 allows not to deal with decimals\n",
        "        self.upsample = torch.nn.Upsample(scale_factor=7)\n",
        "        self.avg_pool = torch.nn.AvgPool2d(kernel_size=(1024 * 7 // 224))\n",
        "\n",
        "    def forward(self, prompt, image):\n",
        "        # text preprocessing\n",
        "        tokenized_prompt = clip.tokenize([prompt]).to(device)\n",
        "        # image preprocessing\n",
        "        preprocessed_image = self.avg_pool(self.upsample(image))\n",
        "        # encoding to CLIP space\n",
        "        text_features = self.model.encode_text(tokenized_prompt)  # Remove .detach()\n",
        "        image_features = self.model.encode_image(preprocessed_image)  # Remove .detach()\n",
        "\n",
        "        # normalize embeddings\n",
        "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "        similarity = (image_features * text_features).sum(dim=1)\n",
        "        loss = 1 - similarity.mean()\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v-9XVx5ys_y"
      },
      "source": [
        "### CLIP Directional loss\n",
        "\n",
        "We want to only change properties described in our text prompt. We use CLIP loss to panish model for changing other properties.\n",
        "\n",
        "The directional loss is computed as follows:\n",
        "\n",
        "$$\n",
        "\\Delta T = E_T(t_{\\text{target}}) - E_T(t_{\\text{source}})\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Delta I = E_I(G_{\\text{train}}(w)) - E_I(G_{\\text{frozen}}(w))\n",
        "$$\n",
        "\n",
        "$$\n",
        "L_{\\text{direction}} = 1 - \\frac{\\Delta I \\cdot \\Delta T}{|\\Delta I| \\cdot |\\Delta T|}\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "\n",
        "- $E_T$: Text encoder function (e.g., CLIP text encoder).\n",
        "- $E_I$: Image encoder function (e.g., CLIP image encoder).\n",
        "- $t_{\\text{target}}$: Target text description.\n",
        "- $t_{\\text{source}}$: Source text description.\n",
        "- $G_{\\text{train}}$: Trainable generator (e.g., StyleGAN during training).\n",
        "- $G_{\\text{frozen}}$: Frozen generator (pre-trained StyleGAN, unmodified during training).\n",
        "- $w$: Latent vector input to the generator.\n",
        "\n",
        "**Intuition:** If the image changes $\\Delta I$ don’t align with the text changes $\\Delta T$, the loss increases, encouraging the generator to better follow the text guidance during optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wewklB2Lz6Cs"
      },
      "outputs": [],
      "source": [
        "class CLIPDirectionalLoss(torch.nn.Module):\n",
        "    def __init__(self, stylegan_size=1024):\n",
        "        super(CLIPDirectionalLoss, self).__init__()\n",
        "\n",
        "        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "        # clip only supports images 224*224.\n",
        "        # we need to resize image with min information loss possible. therefore we firstly apply upsamling, and then pooling.\n",
        "        # scale_factor=7 allows not to deal with decimals\n",
        "        self.upsample = torch.nn.Upsample(scale_factor=7)\n",
        "        self.avg_pool = torch.nn.AvgPool2d(kernel_size=(1024 * 7 // 224))\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        frozen_generator_image,\n",
        "        trainable_generator_image,\n",
        "        source_prompt,\n",
        "        target_prompt,\n",
        "    ):\n",
        "        # text preprocessing\n",
        "        tokenized_source_prompt = clip.tokenize([source_prompt]).to(device)\n",
        "        tokenized_target_prompt = clip.tokenize([target_prompt]).to(device)\n",
        "        # image preprocessing\n",
        "        preprocessed_frozen_generator_image = self.avg_pool(\n",
        "            self.upsample(frozen_generator_image)\n",
        "        )\n",
        "        preprocessed_trainable_generator_image = self.avg_pool(\n",
        "            self.upsample(trainable_generator_image)\n",
        "        )\n",
        "\n",
        "        # encoding to CLIP space\n",
        "        source_prompt_features = self.model.encode_text(tokenized_source_prompt)\n",
        "        target_prompt_features = self.model.encode_text(tokenized_target_prompt)\n",
        "        frozen_generator_image_features = self.model.encode_image(\n",
        "            preprocessed_frozen_generator_image\n",
        "        )\n",
        "        trainable_generator_image_features = self.model.encode_image(\n",
        "            preprocessed_trainable_generator_image\n",
        "        )\n",
        "\n",
        "        delta_t = target_prompt_features - source_prompt_features\n",
        "        delta_i = trainable_generator_image_features - frozen_generator_image_features\n",
        "\n",
        "        # normalize deltas\n",
        "        delta_i = delta_i / delta_i.norm(dim=1, keepdim=True)\n",
        "        delta_t = delta_t / delta_t.norm(dim=1, keepdim=True)\n",
        "\n",
        "        loss = 1 - (delta_i * delta_t).sum(dim=1).mean()\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu5WIuIBBXAy"
      },
      "outputs": [],
      "source": [
        "clip_loss = CLIPLoss()\n",
        "clip_directional_loss = CLIPDirectionalLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5GJ3KnV7eY9"
      },
      "outputs": [],
      "source": [
        "# Параметры генератора\n",
        "size = 1024  # Размер изображения\n",
        "latent_dim = 512  # Размер латентного пространства\n",
        "n_mlp = 8  # Количество слоев MLP\n",
        "channel_multiplier = 2\n",
        "ckpt = \"/content/stylegan2-pytorch/stylegan2-ffhq-config-f.pt\"  # Путь к весам StyleGAN2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1N4GSV67fA7"
      },
      "outputs": [],
      "source": [
        "frozen_generator = Generator(\n",
        "    size, latent_dim, n_mlp, channel_multiplier=channel_multiplier\n",
        ").to(device)\n",
        "trainable_generator = Generator(\n",
        "    size, latent_dim, n_mlp, channel_multiplier=channel_multiplier\n",
        ").to(device)\n",
        "\n",
        "frozen_generator.eval()\n",
        "trainable_generator.train()\n",
        "\n",
        "checkpoint = torch.load(ckpt)\n",
        "\n",
        "frozen_generator.load_state_dict(checkpoint[\"g_ema\"])\n",
        "trainable_generator.load_state_dict(checkpoint[\"g_ema\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omK3R3RY8zKP"
      },
      "outputs": [],
      "source": [
        "def freeze_stylegan_layers(generator, selected_layers=None):\n",
        "    \"\"\"\n",
        "    Freezes specific layers in a StyleGAN generator.\n",
        "\n",
        "    Args:\n",
        "        generator: The StyleGAN generator model.\n",
        "        selected_layers: A list of indices of layers that should remain trainable.\n",
        "                         If None, only Mapping Network, Affine Transformations, and ToRGB layers are frozen.\n",
        "    \"\"\"\n",
        "    # Always freeze Mapping Network (style), ToRGB layers, and Affine Transformations\n",
        "    modules_to_freeze = [\"style\", \"to_rgb1\"]\n",
        "\n",
        "    for module in modules_to_freeze:\n",
        "        if hasattr(generator, module):\n",
        "            for param in getattr(generator, module).parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # Freeze all ToRGB layers in the ModuleList\n",
        "    if hasattr(generator, \"to_rgbs\"):\n",
        "        for to_rgb_layer in generator.to_rgbs:\n",
        "            for param in to_rgb_layer.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # If selected_layers is provided, freeze all other conv layers\n",
        "    if selected_layers is not None:\n",
        "        for i, conv in enumerate(generator.convs):\n",
        "            if i not in selected_layers:  # Freeze layers not in the selected list\n",
        "                for param in conv.parameters():\n",
        "                    param.requires_grad = False\n",
        "            else:\n",
        "                for param in conv.parameters():\n",
        "                    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdDs79pY9L28"
      },
      "outputs": [],
      "source": [
        "freeze_stylegan_layers(frozen_generator)\n",
        "freeze_stylegan_layers(trainable_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jv6IiPNMu4Gy"
      },
      "outputs": [],
      "source": [
        "source_prompt = \"photo\" #@param {\"type\": \"string\"}\n",
        "target_prompt = \"zombie\" #@param {\"type\": \"string\"}\n",
        "\n",
        "# Number of images to use in each step\n",
        "batch_size = 4 #@param {type:\"slider\", min:1, max:4, step:1}\n",
        "num_steps = 60 #@param {type:\"slider\", min:20, max:600, step:10}\n",
        "\n",
        "l2_lambda = 1.0 #@param {type:\"slider\", min:0, max:3, step:0.1}\n",
        "clip_lambda = 2.5 #@param {type:\"slider\", min:0, max:3, step:0.1}\n",
        "clip_directional_lambda = 1.2 #@param {type:\"slider\", min:0, max:3, step:0.1}\n",
        "num_layers = 18\n",
        "\n",
        "optimizer = torch.optim.Adam(trainable_generator.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "def get_k_relevant_layers(generator, k, target_prompt):\n",
        "    # generate random latent vectors (z-space)\n",
        "    latent_z = torch.randn((batch_size, latent_dim), device=device)\n",
        "\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        # map z-space to w-space using the generator's style network\n",
        "        latent_w = generator.style(latent_z).detach()\n",
        "\n",
        "        # expand w-space vectors to w+ by repeating across layers\n",
        "        latent_w_plus = latent_w.unsqueeze(1).repeat(1, num_layers, 1).detach().clone()\n",
        "\n",
        "    # enable gradient tracking for optimization\n",
        "    latent_w_plus.requires_grad_(True)\n",
        "\n",
        "    # store initial state of w+\n",
        "    initial_w_plus = latent_w_plus.clone().detach()\n",
        "\n",
        "    # set up optimizer for w+ updates\n",
        "    optimizer = torch.optim.Adam([latent_w_plus], lr=0.01)\n",
        "\n",
        "    for _ in range(10):  # perform optimization for 10 steps\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # generate image from current latent representation\n",
        "        image, _ = generator([latent_w_plus], input_is_latent=True, randomize_noise=False)\n",
        "\n",
        "        # compute loss based on alignment with target prompt\n",
        "        loss = clip_lambda * clip_loss(target_prompt, image)\n",
        "\n",
        "        # backpropagate and update w+ latents\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # free memory\n",
        "        del image, loss\n",
        "\n",
        "    # compute per-layer changes in w+\n",
        "    delta_w = (latent_w_plus.detach() - initial_w_plus).abs().mean(dim=(0, 2))\n",
        "\n",
        "    # select top-k layers with the most significant changes\n",
        "    relevant_layers = delta_w.topk(k).indices.tolist()\n",
        "    print(\"Relevant layers:\", relevant_layers)\n",
        "\n",
        "    # clean up GPU memory\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return relevant_layers\n",
        "\n",
        "\n",
        "def generate_latents(batch_size, latent_dim, device, generator):\n",
        "    latent_z = torch.randn((batch_size, latent_dim), device=device)\n",
        "    latent_w = generator.style(latent_z).detach().clone()\n",
        "    latent_w_plus = latent_w.unsqueeze(1).repeat(1, num_layers, 1)\n",
        "    return latent_w_plus\n",
        "\n",
        "\n",
        "def generate_images(generator, latent_w_plus):\n",
        "    with torch.no_grad():\n",
        "        images, _ = generator([latent_w_plus], input_is_latent=True, randomize_noise=False)\n",
        "    return images\n",
        "\n",
        "\n",
        "def compute_losses(frozen_images, trainable_images, target_prompt, source_prompt):\n",
        "    l2_loss_value = torch.mean((frozen_images - trainable_images) ** 2)\n",
        "    clip_loss_value = clip_loss(target_prompt, trainable_images)\n",
        "    clip_directional_loss_value = clip_directional_loss(\n",
        "        frozen_images, trainable_images, source_prompt, target_prompt\n",
        "    )\n",
        "    return l2_loss_value, clip_loss_value, clip_directional_loss_value\n",
        "\n",
        "\n",
        "def log_losses(losses, l2_loss_value, clip_loss_value, clip_directional_loss_value, loss):\n",
        "    losses[\"l2\"].append(l2_loss_value.detach().item())\n",
        "    losses[\"clip\"].append(clip_loss_value.detach().item())\n",
        "    losses[\"clip_directional\"].append(clip_directional_loss_value.detach().item())\n",
        "    losses[\"total\"].append(loss.detach().item())\n",
        "\n",
        "\n",
        "def visualize_images(frozen_images, trainable_images, batch_size, step, num_steps, losses):\n",
        "    visualize_interval = 20 #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "    if step % 20 == 0 or step == num_steps - 1:\n",
        "        print(f\"Step [{step}/{num_steps}], Loss: {losses['total'][-1]}\")\n",
        "        fig, axs = plt.subplots(2, batch_size, figsize=(batch_size * 4, 8))\n",
        "        if batch_size == 1:\n",
        "            axs = [[axs[0]], [axs[1]]]\n",
        "        for i in range(batch_size):\n",
        "            axs[0][i].imshow(((frozen_images.cpu().detach()[i].clip(-1, 1).permute(1, 2, 0) + 1) / 2))\n",
        "            axs[0][i].set_title(f\"Frozen Image {i+1}\")\n",
        "            axs[0][i].axis(\"off\")\n",
        "            axs[1][i].imshow(((trainable_images.cpu().detach()[i].clip(-1, 1).permute(1, 2, 0) + 1) / 2))\n",
        "            axs[1][i].set_title(f\"Trainable Image {i+1}\")\n",
        "            axs[1][i].axis(\"off\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "losses = {\"l2\": [], \"clip\": [], \"clip_directional\": [], \"total\": []}\n",
        "\n",
        "def plot_losses(losses):\n",
        "    \"\"\"Plots the given losses over steps in a 2x2 grid.\"\"\"\n",
        "    steps = range(len(losses[\"total\"]))\n",
        "\n",
        "    # define subplot parameters\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "    fig.suptitle(\"Losses Over Steps\", fontsize=16)\n",
        "\n",
        "    # define loss titles and corresponding keys\n",
        "    loss_info = [\n",
        "        (\"L2 Loss\", \"l2\"),\n",
        "        (\"CLIP Loss\", \"clip\"),\n",
        "        (\"CLIP Directional\", \"clip_directional\"),\n",
        "        (\"Total Loss\", \"total\")\n",
        "    ]\n",
        "\n",
        "    # plot each loss in its respective subplot\n",
        "    for ax, (title, key) in zip(axs.flat, loss_info):\n",
        "        ax.plot(steps, losses[key], marker=\"o\", label=f\"{key.upper()} Loss\")\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlabel(\"Step\")\n",
        "        ax.set_ylabel(\"Loss\")\n",
        "        ax.grid(True)\n",
        "        ax.legend()\n",
        "\n",
        "    # adjust layout and display\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "for step in range(num_steps):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # freeze all layers, but 5 relevant\n",
        "    layer_freezing_interval = 30 #@param {type:\"slider\", min:10, max:100, step:10}\n",
        "    trainable_layers_number = 5 #@param {type:\"slider\", min:2, max:18, step:1}\n",
        "    if step % 20 == 0:\n",
        "        k_relevant_layers = get_k_relevant_layers(trainable_generator, 5, target_prompt)\n",
        "        freeze_stylegan_layers(trainable_generator, k_relevant_layers)\n",
        "\n",
        "    # get latents from w+\n",
        "    latent_w_plus = generate_latents(batch_size, latent_dim, device, frozen_generator)\n",
        "\n",
        "    # generate images with frozen and trainable generators\n",
        "    frozen_generator_images = generate_images(frozen_generator, latent_w_plus)\n",
        "    trainable_generator_images = generate_images(trainable_generator, latent_w_plus)\n",
        "\n",
        "    # get all losses\n",
        "    l2_loss_value, clip_loss_value, clip_directional_loss_value = compute_losses(\n",
        "        frozen_generator_images, trainable_generator_images, target_prompt, source_prompt\n",
        "    )\n",
        "\n",
        "    # evaluate total weighted loss\n",
        "    loss = (\n",
        "        l2_lambda * l2_loss_value\n",
        "        + clip_lambda * clip_loss_value\n",
        "        + clip_directional_lambda * clip_directional_loss_value\n",
        "    )\n",
        "    log_losses(losses, l2_loss_value, clip_loss_value, clip_directional_loss_value, loss)\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    visualize_images(frozen_generator_images, trainable_generator_images, batch_size, step, num_steps, losses)\n",
        "    display_loss_plot = True #@param {type:\"boolean\"}\n",
        "    if display_loss_plot == True:\n",
        "        plot_losses(losses)\n",
        "\n",
        "    # free memory\n",
        "    del frozen_generator_images, trainable_generator_images, latent_w_plus\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRbzBgChiF3m"
      },
      "source": [
        "Inversion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzGKu0AciSMA"
      },
      "outputs": [],
      "source": [
        "# Определяем преобразования для изображения\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((256, 256)),  # Изменяем размер изображения до 256x256\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
        "        ),  # Нормализуем значения пикселей\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uqUFYaIijGl"
      },
      "outputs": [],
      "source": [
        "# Загружаем предобученную модель pSp\n",
        "model_path = \"/content/encoder4editing/e4e_ffhq_encode.pt\"  # Путь к весам энкодера\n",
        "ckpt = torch.load(model_path, map_location=\"cpu\")\n",
        "opts = ckpt[\"opts\"]\n",
        "\n",
        "opts[\"checkpoint_path\"] = model_path\n",
        "opts = Namespace(**opts)\n",
        "net = pSp(opts)\n",
        "net.eval()\n",
        "net.to(device)\n",
        "print(\"Model successfully loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Avz-e8xMi8VI"
      },
      "outputs": [],
      "source": [
        "image_path = \"/content/images/mikita.jpeg\"  # Путь к изображению\n",
        "original_image = Image.open(image_path)\n",
        "original_image = original_image.convert(\"RGB\")  # Преобразуем изображение в RGB формат\n",
        "original_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of10C4bki-Sq"
      },
      "outputs": [],
      "source": [
        "# Определяем тип эксперимента\n",
        "experiment_type = \"ffhq_encode\"\n",
        "\n",
        "# Загружаем модель для определения ключевых точек лица - понадобится нам для выравнивания\n",
        "if experiment_type == \"ffhq_encode\" and 'shape_predictor_68_face_landmarks.dat' not in os.listdir():\n",
        "    !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2  # Скачиваем архив с моделью\n",
        "    !bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2  # Распаковываем архив\n",
        "\n",
        "# Задаем размер изображения для изменения\n",
        "resize_dims = (256, 256)\n",
        "\n",
        "\n",
        "def run_alignment(image_path):\n",
        "  \"\"\"Выравнивает лицо на изображении, используя dlib.\n",
        "\n",
        "  Args:\n",
        "      image_path: Путь к файлу изображения.\n",
        "\n",
        "  Returns:\n",
        "      Выровненное изображение в формате PIL.Image.\n",
        "  \"\"\"\n",
        "  import dlib\n",
        "  from utils.alignment import align_face  # Функция для выравнивания лица\n",
        "  predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "  aligned_image = align_face(filepath=image_path, predictor=predictor)\n",
        "  print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
        "  return aligned_image  # Возвращаем выровненное изображение\n",
        "\n",
        "# Выравниваем лицо, если тип эксперимента - кодирование FFHQ\n",
        "if experiment_type == \"ffhq_encode\":\n",
        "  input_image = run_alignment(image_path)\n",
        "else:\n",
        "  input_image = original_image\n",
        "\n",
        "\n",
        "input_image.resize(resize_dims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxoc9UVSlQAk"
      },
      "outputs": [],
      "source": [
        "transformed_image = transform(input_image)\n",
        "\n",
        "def run_on_batch(inputs, net):\n",
        "    \"\"\"Запускает модель на пакете данных.\n",
        "\n",
        "    Args:\n",
        "        inputs: Входные данные (тензор PyTorch).\n",
        "        net: Модель pSp.\n",
        "\n",
        "    Returns:\n",
        "        Сгенерированные изображения и латентные векторы.\n",
        "    \"\"\"\n",
        "    images, latents = net(\n",
        "        inputs.to(\"cuda\").float(), randomize_noise=False, return_latents=True\n",
        "    )  # Запускаем модель\n",
        "    return images, latents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5Yx8YBdlS8Q"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    start = time.time()\n",
        "    result_image, latent = run_on_batch(transformed_image.unsqueeze(0), net)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent.shape"
      ],
      "metadata": {
        "id": "Nlh51G1lDDf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO_ntbUcquai"
      },
      "outputs": [],
      "source": [
        "trainable_generator.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d7VmfPaqso-"
      },
      "outputs": [],
      "source": [
        "trainable_generator_images, _ = trainable_generator(\n",
        "    [latent], input_is_latent=True, randomize_noise=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laGu5VUPih3C"
      },
      "source": [
        "Cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "TjrgQs3tDui9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z9bNEVV1m0N"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Преобразуем формат (C, H, W) → (H, W, C)\n",
        "image = trainable_generator_images[0].detach().cpu().permute(1, 2, 0).numpy()\n",
        "\n",
        "axs[0].imshow(np.asarray(original_image))\n",
        "axs[0].set_title(\"Исходное изображение\")\n",
        "axs[0].axis(\"off\")\n",
        "\n",
        "axs[1].imshow(image)\n",
        "axs[1].set_title(\"Обработанное\")\n",
        "axs[1].axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}