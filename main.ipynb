{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikita-resh/StyleGAN-NADA/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1IrCWriCkn_"
      },
      "outputs": [],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install Ninja\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "import clip\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRJKLefO6Q3Y"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/rosinality/stylegan2-pytorch.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK8GkzSI6XSw"
      },
      "outputs": [],
      "source": [
        "%cd stylegan2-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAS8Psrt6g_d"
      },
      "outputs": [],
      "source": [
        "from model import Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDFlXuGM6sbV"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/stylegan2-pytorch/lpips/__init__.py\"\n",
        "\n",
        "# Откроем файл и прочитаем его содержимое\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Заменим нужную строку\n",
        "with open(file_path, 'w') as file:\n",
        "    for line in lines:\n",
        "        # Если в строке есть \"compare_ssim\", заменим на \"structural_similarity\"\n",
        "        if \"from skimage.measure import compare_ssim\" in line:\n",
        "            file.write(\"from skimage.metrics import structural_similarity\\n\")\n",
        "        else:\n",
        "            file.write(line)\n",
        "\n",
        "print(\"Замена выполнена успешно!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPIfAAk-7E9-"
      },
      "outputs": [],
      "source": [
        "# Зададим девайс: cuda или cpu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAuBIaWQ7N6_"
      },
      "outputs": [],
      "source": [
        "# URL for the Google Drive file\n",
        "url = 'https://drive.google.com/uc?id=1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT'\n",
        "\n",
        "# Download the file\n",
        "gdown.download(url, 'stylegan2-ffhq-config-f.pt', quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DswhREmsUm2"
      },
      "source": [
        "### CLIP Loss\n",
        "\n",
        "We want the model to generate images that align with the properties described in the text prompt. To achieve this, we use CLIP loss to encourage the model to generate images that are closer to the text description in the CLIP embedding space.\n",
        "\n",
        "The CLIP loss is computed as follows:\n",
        "\n",
        "$$\n",
        "D_{\\text{CLIP}} = 1 - \\frac{\\langle E_T(G(w)) \\cdot E_T(t) \\rangle }{\\|E_T(G(w))\\| \\cdot \\|E_T(t)\\|}\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "\n",
        "- $E_T$: Text and image encoder function (e.g., CLIP's shared encoder for both text and image).\n",
        "- $t$: Text description (prompt).\n",
        "- $G(w)$: Image generated by the generator using the latent vector $w$.\n",
        "- $D_{\\text{CLIP}}$: Cosine distance between the text embedding and the image embedding in the CLIP space.\n",
        "\n",
        "**Intuition:** The CLIP loss encourages the generated image $G(w)$ to be semantically aligned with the provided text prompt $t$. If the image and text embeddings are close in the CLIP space, the loss will be small, helping the model generate images that reflect the described properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sc72kwOkJRN"
      },
      "outputs": [],
      "source": [
        "class CLIPLoss(torch.nn.Module):\n",
        "  def __init__(self, stylegan_size=1024):\n",
        "    super(CLIPLoss, self).__init__()\n",
        "\n",
        "    self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "    # clip only supports images 224*224.\n",
        "    # we need to resize image with min information loss possible. therefore we firstly apply upsamling, and then pooling.\n",
        "    # scale_factor=7 allows not to deal with decimals\n",
        "    self.upsample = torch.nn.Upsample(scale_factor=7)\n",
        "    self.avg_pool = torch.nn.AvgPool2d(kernel_size=(1024*7//224))\n",
        "\n",
        "  def forward(self, prompt, image):\n",
        "    # text preprocessing\n",
        "    tokenized_prompt = clip.tokenize([prompt]).to(device)\n",
        "    # image preprocessing\n",
        "    preprocessed_image = self.avg_pool(self.upsample(image))\n",
        "\n",
        "    # encoding to CLIP space\n",
        "    text_features = self.model.encode_text(tokenized_prompt)\n",
        "    image_features = self.model.encode_image(preprocessed_image)\n",
        "\n",
        "    # normalize embeddings\n",
        "    image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
        "    text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "    similarity = (image_features * text_features).sum(dim=1)\n",
        "    loss = 1 - similarity.mean()\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v-9XVx5ys_y"
      },
      "source": [
        "### CLIP Directional loss\n",
        "\n",
        "We want to only change properties described in our text prompt. We use CLIP loss to panish model for changing other properties.\n",
        "\n",
        "The directional loss is computed as follows:\n",
        "\n",
        "$$\n",
        "\\Delta T = E_T(t_{\\text{target}}) - E_T(t_{\\text{source}})\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Delta I = E_I(G_{\\text{train}}(w)) - E_I(G_{\\text{frozen}}(w))\n",
        "$$\n",
        "\n",
        "$$\n",
        "L_{\\text{direction}} = 1 - \\frac{\\Delta I \\cdot \\Delta T}{|\\Delta I| \\cdot |\\Delta T|}\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "\n",
        "- $E_T$: Text encoder function (e.g., CLIP text encoder).\n",
        "- $E_I$: Image encoder function (e.g., CLIP image encoder).\n",
        "- $t_{\\text{target}}$: Target text description.\n",
        "- $t_{\\text{source}}$: Source text description.\n",
        "- $G_{\\text{train}}$: Trainable generator (e.g., StyleGAN during training).\n",
        "- $G_{\\text{frozen}}$: Frozen generator (pre-trained StyleGAN, unmodified during training).\n",
        "- $w$: Latent vector input to the generator.\n",
        "\n",
        "**Intuition:** If the image changes $\\Delta I$ don’t align with the text changes $\\Delta T$, the loss increases, encouraging the generator to better follow the text guidance during optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wewklB2Lz6Cs"
      },
      "outputs": [],
      "source": [
        "class CLIPDirectionalLoss(torch.nn.Module):\n",
        "  def __init__(self, stylegan_size=1024):\n",
        "    super(CLIPDirectionalLoss, self).__init__()\n",
        "\n",
        "    self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "    # clip only supports images 224*224.\n",
        "    # we need to resize image with min information loss possible. therefore we firstly apply upsamling, and then pooling.\n",
        "    # scale_factor=7 allows not to deal with decimals\n",
        "    self.upsample = torch.nn.Upsample(scale_factor=7)\n",
        "    self.avg_pool = torch.nn.AvgPool2d(kernel_size=(1024*7//224))\n",
        "\n",
        "  def forward(self, frozen_generator_image, trainable_generator_image, source_prompt, target_prompt):\n",
        "    # text preprocessing\n",
        "    tokenized_source_prompt = clip.tokenize([source_prompt]).to(device)\n",
        "    tokenized_target_prompt = clip.tokenize([target_prompt]).to(device)\n",
        "    # image preprocessing\n",
        "    preprocessed_frozen_generator_image = self.avg_pool(self.upsample(frozen_generator_image))\n",
        "    preprocessed_trainable_generator_image = self.avg_pool(self.upsample(trainable_generator_image))\n",
        "\n",
        "    # encoding to CLIP space\n",
        "    source_prompt_features = self.model.encode_text(tokenized_source_prompt)\n",
        "    target_prompt_features = self.model.encode_text(tokenized_target_prompt)\n",
        "    frozen_generator_image_features = self.model.encode_image(preprocessed_frozen_generator_image)\n",
        "    trainable_generator_image_features = self.model.encode_image(preprocessed_trainable_generator_image)\n",
        "\n",
        "    delta_t = target_prompt_features - source_prompt_features\n",
        "    delta_i = trainable_generator_image_features - frozen_generator_image_features\n",
        "\n",
        "    # normalize deltas\n",
        "    delta_i = delta_i / delta_i.norm(dim=1, keepdim=True)\n",
        "    delta_t = delta_t / delta_t.norm(dim=1, keepdim=True)\n",
        "\n",
        "    loss = 1 - (delta_i * delta_t).sum(dim=1).mean()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu5WIuIBBXAy"
      },
      "outputs": [],
      "source": [
        "clip_loss = CLIPLoss()\n",
        "clip_directional_loss = CLIPDirectionalLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5GJ3KnV7eY9"
      },
      "outputs": [],
      "source": [
        "# Параметры генератора\n",
        "size = 1024  # Размер изображения\n",
        "latent_dim = 512  # Размер латентного пространства\n",
        "n_mlp = 8  # Количество слоев MLP\n",
        "channel_multiplier = 2\n",
        "ckpt = '/content/stylegan2-pytorch/stylegan2-ffhq-config-f.pt'  # Путь к весам StyleGAN2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1N4GSV67fA7"
      },
      "outputs": [],
      "source": [
        "frozen_generator = Generator(size, latent_dim, n_mlp, channel_multiplier=channel_multiplier).to(device)\n",
        "trainable_generator = Generator(size, latent_dim, n_mlp, channel_multiplier=channel_multiplier).to(device)\n",
        "\n",
        "frozen_generator.eval()\n",
        "trainable_generator.train()\n",
        "\n",
        "checkpoint = torch.load(ckpt)\n",
        "\n",
        "frozen_generator.load_state_dict(checkpoint[\"g_ema\"])\n",
        "trainable_generator.load_state_dict(checkpoint[\"g_ema\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSPVmts1G_hV"
      },
      "outputs": [],
      "source": [
        "batch_size = 3  # Number of images to use in each step\n",
        "num_steps = 201\n",
        "\n",
        "l2_lambda = 1.0\n",
        "clip_lambda = 1.5\n",
        "clip_directional_lambda = 1\n",
        "id_lambda = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(trainable_generator.parameters(), lr=0.01)\n",
        "\n",
        "source_prompt = \"photo\"\n",
        "target_prompt = \"sketch\"\n",
        "\n",
        "losses = {'l2': [], 'clip': [], 'clip_directional': [], 'total': []}\n",
        "\n",
        "for step in range(num_steps):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Generate a batch of latent vectors\n",
        "    latent_z = torch.randn((batch_size, latent_dim), device=device)\n",
        "    latent_w = frozen_generator.style(latent_z).detach().clone()\n",
        "\n",
        "    # Generate images for the batch\n",
        "    frozen_generator_images, _ = frozen_generator([latent_w], input_is_latent=True, randomize_noise=False)\n",
        "    trainable_generator_images, _ = trainable_generator([latent_w], input_is_latent=True, randomize_noise=False)\n",
        "\n",
        "    # Compute the losses for the batch\n",
        "    l2_loss_value = torch.mean((frozen_generator_images - trainable_generator_images) ** 2)\n",
        "    clip_loss_value = clip_loss(target_prompt, trainable_generator_images)\n",
        "    clip_directional_loss_value = clip_directional_loss(\n",
        "        frozen_generator_images, trainable_generator_images, source_prompt, target_prompt\n",
        "    )\n",
        "\n",
        "    # Combine losses\n",
        "    loss = (\n",
        "        l2_lambda * l2_loss_value\n",
        "        + clip_lambda * clip_loss_value\n",
        "        + clip_directional_lambda * clip_directional_loss_value\n",
        "    )\n",
        "\n",
        "    # Log losses\n",
        "    losses['l2'].append(l2_loss_value.detach().item())\n",
        "    losses['clip'].append(clip_loss_value.detach().item())\n",
        "    losses['clip_directional'].append(clip_directional_loss_value.detach().item())\n",
        "    losses['total'].append(loss.detach().item())\n",
        "\n",
        "    # Backpropagation and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Visualization and logging\n",
        "    if step % 20 == 0 or step == num_steps - 1:\n",
        "        print(f\"Step [{step}/{num_steps}], Loss: {losses['total'][-1]}\")\n",
        "\n",
        "        # Create a horizontal layout with 1 row and batch_size columns\n",
        "        fig, axs = plt.subplots(1, batch_size, figsize=(batch_size * 4, 4))  # Adjust figsize for better spacing\n",
        "\n",
        "        # If batch_size = 1, axs will not be iterable, so wrap it in a list\n",
        "        if batch_size == 1:\n",
        "            axs = [axs]\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            axs[i].imshow((trainable_generator_images.cpu().detach()[i].clip(-1, 1).permute(1, 2, 0) + 1) / 2)\n",
        "            axs[i].set_title(f\"Image {i+1}\")\n",
        "            axs[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_OBEj4sPjs3"
      },
      "outputs": [],
      "source": [
        "# Steps corresponding to the losses\n",
        "steps = range(len(losses['total']))\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "fig.suptitle('Losses Over Steps', fontsize=16)\n",
        "\n",
        "# Titles and keys for the losses\n",
        "titles = [ 'L2 Loss', 'CLIP Loss', 'CLIP Directional' 'Total Loss']\n",
        "loss_keys = ['l2', 'clip', 'clip_directional', 'total']\n",
        "\n",
        "# Plot each loss\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    loss_key = loss_keys[i]\n",
        "    ax.plot(steps, losses[loss_key], marker='o', label=f'{loss_key.upper()} Loss')\n",
        "    ax.set_title(titles[i])\n",
        "    ax.set_xlabel('Step')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "\n",
        "# Adjust layout to fit the title and spacing\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Move tensors to CPU and delete them\n",
        "del frozen_generator  # Assuming `model` is your trained model\n",
        "del trainable_generator  # Assuming `model` is your trained model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Collect garbage to clear references\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "ijUskZ8ZWUk8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyP7qYm2uPEU7EcaFJrxDTGz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}