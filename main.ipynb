{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpPHV7S7Mw9iYTBvr1iY+s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikita-resh/StyleGAN-NADA/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install Ninja\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "import clip"
      ],
      "metadata": {
        "id": "X1IrCWriCkn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rosinality/stylegan2-pytorch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRJKLefO6Q3Y",
        "outputId": "0a3cba2a-9e1c-42cd-d63a-32bfc49f7d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan2-pytorch'...\n",
            "remote: Enumerating objects: 395, done.\u001b[K\n",
            "remote: Total 395 (delta 0), reused 0 (delta 0), pack-reused 395 (from 1)\u001b[K\n",
            "Receiving objects: 100% (395/395), 122.51 MiB | 14.83 MiB/s, done.\n",
            "Resolving deltas: 100% (203/203), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd stylegan2-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WK8GkzSI6XSw",
        "outputId": "a342d1ce-3221-469d-e429-b8f782a23b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stylegan2-pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from model import Generator"
      ],
      "metadata": {
        "id": "GAS8Psrt6g_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/stylegan2-pytorch/lpips/__init__.py\"\n",
        "\n",
        "# Откроем файл и прочитаем его содержимое\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Заменим нужную строку\n",
        "with open(file_path, 'w') as file:\n",
        "    for line in lines:\n",
        "        # Если в строке есть \"compare_ssim\", заменим на \"structural_similarity\"\n",
        "        if \"from skimage.measure import compare_ssim\" in line:\n",
        "            file.write(\"from skimage.metrics import structural_similarity\\n\")\n",
        "        else:\n",
        "            file.write(line)\n",
        "\n",
        "print(\"Замена выполнена успешно!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDFlXuGM6sbV",
        "outputId": "aebe6b10-4d44-4350-e9a1-43d283143a8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Замена выполнена успешно!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Зададим девайс: cuda или cpu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "DPIfAAk-7E9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URL for the Google Drive file\n",
        "url = 'https://drive.google.com/uc?id=1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT'\n",
        "\n",
        "# Download the file\n",
        "gdown.download(url, 'stylegan2-ffhq-config-f.pt', quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "DAuBIaWQ7N6_",
        "outputId": "3393a458-2e7a-4d19-ddd4-25dd9db1b785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT\n",
            "From (redirected): https://drive.google.com/uc?id=1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT&confirm=t&uuid=454e82f2-c43e-4340-8b6a-fc55dfebcaed\n",
            "To: /content/stylegan2-pytorch/stylegan2-ffhq-config-f.pt\n",
            "100%|██████████| 381M/381M [00:14<00:00, 26.9MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'stylegan2-ffhq-config-f.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLIP Loss\n",
        "\n",
        "We want the model to generate images that align with the properties described in the text prompt. To achieve this, we use CLIP loss to encourage the model to generate images that are closer to the text description in the CLIP embedding space.\n",
        "\n",
        "The CLIP loss is computed as follows:\n",
        "\n",
        "$$\n",
        "D_{\\text{CLIP}} = 1 - \\frac{\\langle E_T(G(w)) \\cdot E_T(t) \\rangle }{\\|E_T(G(w))\\| \\cdot \\|E_T(t)\\|}\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "\n",
        "- $E_T$: Text and image encoder function (e.g., CLIP's shared encoder for both text and image).\n",
        "- $t$: Text description (prompt).\n",
        "- $G(w)$: Image generated by the generator using the latent vector $w$.\n",
        "- $D_{\\text{CLIP}}$: Cosine distance between the text embedding and the image embedding in the CLIP space.\n",
        "\n",
        "**Intuition:** The CLIP loss encourages the generated image $G(w)$ to be semantically aligned with the provided text prompt $t$. If the image and text embeddings are close in the CLIP space, the loss will be small, helping the model generate images that reflect the described properties."
      ],
      "metadata": {
        "id": "-DswhREmsUm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPLoss(torch.nn.Module):\n",
        "  def __init__(self, stylegan_size=1024):\n",
        "    super(CLIPLoss, self).__init__()\n",
        "\n",
        "    self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "    # clip only supports images 224*224.\n",
        "    # we need to resize image with min information loss possible. therefore we firstly apply upsamling, and then pooling.\n",
        "    # scale_factor=7 allows not to deal with decimals\n",
        "    self.upsample = torch.nn.Upsample(scale_factor=7)\n",
        "    self.avg_pool = torch.nn.AvgPool2d(kernel_size=(1024*7/224))\n",
        "\n",
        "  def forward(self, prompt, image):\n",
        "    # text preprocessing\n",
        "    tokenized_prompt = clip.tokenize([prompt]).to(device)\n",
        "    # image preprocessing\n",
        "    preprocessed_image = self.avg_pool(self.upsample(image))\n",
        "\n",
        "    # encoding to CLIP space\n",
        "    text_features = model.encode_text(tokenized_prompt)\n",
        "    image_features = model.encode_image(preprocessed_image)\n",
        "\n",
        "    # normalize embeddings\n",
        "    image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
        "    text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "    similarity = (image_features * text_features).sum(dim=1)\n",
        "    loss = 1 - similarity.mean()\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "0sc72kwOkJRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLIP Directional loss\n",
        "\n",
        "We want to only change properties described in our text prompt. We use CLIP loss to panish model for changing other properties.\n",
        "\n",
        "The directional loss is computed as follows:\n",
        "\n",
        "$$\n",
        "\\Delta T = E_T(t_{\\text{target}}) - E_T(t_{\\text{source}})\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Delta I = E_I(G_{\\text{train}}(w)) - E_I(G_{\\text{frozen}}(w))\n",
        "$$\n",
        "\n",
        "$$\n",
        "L_{\\text{direction}} = 1 - \\frac{\\Delta I \\cdot \\Delta T}{|\\Delta I| \\cdot |\\Delta T|}\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "\n",
        "- $E_T$: Text encoder function (e.g., CLIP text encoder).\n",
        "- $E_I$: Image encoder function (e.g., CLIP image encoder).\n",
        "- $t_{\\text{target}}$: Target text description.\n",
        "- $t_{\\text{source}}$: Source text description.\n",
        "- $G_{\\text{train}}$: Trainable generator (e.g., StyleGAN during training).\n",
        "- $G_{\\text{frozen}}$: Frozen generator (pre-trained StyleGAN, unmodified during training).\n",
        "- $w$: Latent vector input to the generator.\n",
        "\n",
        "**Intuition:** If the image changes $\\Delta I$ don’t align with the text changes $\\Delta T$, the loss increases, encouraging the generator to better follow the text guidance during optimization."
      ],
      "metadata": {
        "id": "7v-9XVx5ys_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPDirectionalLoss(torch.nn.Module):\n",
        "  def __init__(self.stylegan_size=1024):\n",
        "    super(CLIPDirectionalLoss, self).__init__()\n",
        "\n",
        "    self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "    # clip only supports images 224*224.\n",
        "    # we need to resize image with min information loss possible. therefore we firstly apply upsamling, and then pooling.\n",
        "    # scale_factor=7 allows not to deal with decimals\n",
        "    self.upsample = torch.nn.Upsample(scale_factor=7)\n",
        "    self.avg_pool = torch.nn.AvgPool2d(kernel_size=(1024*7/224))\n",
        "\n",
        "  def forward(self, frozen_generator_image, trainable_generator_image, source_prompt, target_prompt):\n",
        "    # text preprocessing\n",
        "    tokenized_source_prompt = clip.tokenize([source_prompt]).to(device)\n",
        "    tokenized_target_prompt = clip.tokenize([target_prompt]).to(device)\n",
        "    # image preprocessing\n",
        "    preprocessed_frozen_generator_image = self.avg_pool(self.upsample(frozen_generator_image))\n",
        "    preprocessed_trainable_generator_image = self.avg_pool(self.upsample(trainable_generator_image))\n",
        "\n",
        "    # encoding to CLIP space\n",
        "    source_prompt_features = model.encode_text(tokenized_source_prompt)\n",
        "    target_prompt_features = model.encode_text(tokenized_target_prompt)\n",
        "    frozen_generator_image_features = model.encode_image(preprocessed_frozen_generator_image)\n",
        "    trainable_generator_image_features = model.encode_image(preprocessed_trainable_generator_image)\n",
        "\n",
        "    delta_t = target_prompt_features - source_prompt_features\n",
        "    delta_i = trainable_generator_image_features - frozen_generator_image_features\n",
        "\n",
        "    # normalize deltas\n",
        "    delta_i = delta_i / delta_i.norm(dim=1, keepdim=True)\n",
        "    delta_t = delta_t / delta_t.norm(dim=1, keepdim=True)\n",
        "\n",
        "    loss = 1 - (delta_i * delta_t).sum(dim=1).mean()\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "wewklB2Lz6Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_loss = CLIPLoss()\n",
        "clip_directional_loss = CLIPDirectionalLoss()"
      ],
      "metadata": {
        "id": "uu5WIuIBBXAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры генератора\n",
        "size = 1024  # Размер изображения\n",
        "latent_dim = 512  # Размер латентного пространства\n",
        "n_mlp = 8  # Количество слоев MLP\n",
        "channel_multiplier = 2\n",
        "ckpt = '/content/stylegan2-pytorch/stylegan2-ffhq-config-f.pt'  # Путь к весам StyleGAN2"
      ],
      "metadata": {
        "id": "B5GJ3KnV7eY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frozen_generator = Generator(size, latent_dim, n_mlp, channel_multiplier=channel_multiplier).to(device)\n",
        "trainable_generator = Generator(size, latent_dim, n_mlp, channel_multiplier=channel_multiplier).to(device)\n",
        "\n",
        "frozen_generator.eval()\n",
        "trainable_generator.train()\n",
        "\n",
        "checkpoint = torch.load(ckpt)\n",
        "\n",
        "frozen_generator.load_state_dict(checkpoint[\"g_ema\"])\n",
        "trainable_generator.load_state_dict(checkpoint[\"g_ema\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1N4GSV67fA7",
        "outputId": "62ea7e5b-f47f-43f5-dc49-019240c46730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-b92cbebfb1e2>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(ckpt)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps = 201\n",
        "\n",
        "l2_lambda = 0.01\n",
        "clip_lambda = 1.5\n",
        "clip_directional_lambda = 1\n",
        "id_lambda = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(trainable_generator.parameters(), lr=0.08)\n",
        "\n",
        "source_prompt = \"photo\"\n",
        "target_prompt = \"sketch\"\n",
        "\n",
        "losses = {'l2': [], 'clip': [], 'clip_directional': [], 'total': []}\n",
        "\n",
        "for step in range(num_steps):\n",
        "  latent_z = torch.randn((1, latent_dim), device=device)\n",
        "  latent_w = frozen_generator.style(latent_z).detach().clone()\n",
        "\n",
        "  frozen_generator_image, _ = frozen_generator([latent_w], input_is_latent=True, randomize_noise=False)\n",
        "  trainable_generator_image, _ = trainable_generator([latent_w], input_is_latent=True, randomize_noise=False)\n",
        "\n",
        "  l2_loss_value = torch.sum((frozen_generator_image - trainable_generator_image) ** 2)\n",
        "  clip_loss_value = clip_loss(target_prompt, trainable_generator_image)\n",
        "  clip_directional_loss_value = clip_directional_loss(frozen_generator_image, trainable_generator_image, source_prompt, target_prompt)\n",
        "  loss = l2_lambda * l2_loss_value + clip_lambda * clip_loss_value + clip_directional_lambda * clip_directional_loss_value\n",
        "\n",
        "  losses['l2'].append(l2_loss_value.detach().item())\n",
        "  losses['clip'].append(clip_loss_value.detach().item())\n",
        "  losses['clip_directional'].append(clip_directional_loss_value.detach().item())\n",
        "  losses['total'].append(loss.detach().item())\n",
        "\n",
        "  if step % 5 == 0 or step == num_steps -1:\n",
        "    print(f\"Step [{step}/{num_steps}], Loss: {losses['all'][-1]}\")\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "    axs[0].imshow((frozen_generator_image.cpu().detach()[0].clip(-1, 1).permute(1, 2, 0) + 1) / 2)\n",
        "    axs[0].set_title(\"Frozen Genarator Image\")\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    axs[1].imshow((trainable_generator_image.cpu().detach()[0].clip(-1, 1).permute(1, 2, 0) + 1) / 2)\n",
        "    axs[1].set_title(\"Trainable Generator Image\")\n",
        "    axs[1].axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "sSPVmts1G_hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Steps corresponding to the losses\n",
        "steps = range(len(losses['id']))\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "fig.suptitle('Losses Over Steps', fontsize=16)\n",
        "\n",
        "# Titles and keys for the losses\n",
        "titles = [ 'L2 Loss', 'CLIP Loss', 'CLIP Directional' 'Total Loss']\n",
        "loss_keys = ['l2', 'clip', 'clip_directional', 'total']\n",
        "\n",
        "# Plot each loss\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    loss_key = loss_keys[i]\n",
        "    ax.plot(steps, losses[loss_key], marker='o', label=f'{loss_key.upper()} Loss')\n",
        "    ax.set_title(titles[i])\n",
        "    ax.set_xlabel('Step')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "\n",
        "# Adjust layout to fit the title and spacing\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e_OBEj4sPjs3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}