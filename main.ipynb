{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikita-resh/StyleGAN-NADA/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1IrCWriCkn_",
        "outputId": "0d0affa5-6ca0-4d71-a37f-e63b5b712e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-n32blp3j\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-n32blp3j\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.20.1+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=33532e48cc47f9edfdd18d62f4b347fc90d317db762fe79fcb2399a73ac14831\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jx8i9eht/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
            "Successfully built clip\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed clip-1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting Ninja\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Ninja\n",
            "Successfully installed Ninja-1.11.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install Ninja\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "import clip\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRJKLefO6Q3Y",
        "outputId": "197da677-9bdd-400f-c97d-783fbb8a593b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan2-pytorch'...\n",
            "remote: Enumerating objects: 395, done.\u001b[K\n",
            "remote: Total 395 (delta 0), reused 0 (delta 0), pack-reused 395 (from 1)\u001b[K\n",
            "Receiving objects: 100% (395/395), 122.51 MiB | 25.23 MiB/s, done.\n",
            "Resolving deltas: 100% (205/205), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rosinality/stylegan2-pytorch.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WK8GkzSI6XSw",
        "outputId": "2eea7e3c-c256-42dd-bd96-53dfe8b9acb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stylegan2-pytorch\n"
          ]
        }
      ],
      "source": [
        "%cd stylegan2-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAS8Psrt6g_d",
        "outputId": "006d9583-24d2-44a6-9a79-99b0e11f2aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from model import Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDFlXuGM6sbV",
        "outputId": "6a9a1c00-c8b7-4902-83d7-931a248bc4ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Замена выполнена успешно!\n"
          ]
        }
      ],
      "source": [
        "file_path = \"/content/stylegan2-pytorch/lpips/__init__.py\"\n",
        "\n",
        "# Откроем файл и прочитаем его содержимое\n",
        "with open(file_path, \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Заменим нужную строку\n",
        "with open(file_path, \"w\") as file:\n",
        "    for line in lines:\n",
        "        # Если в строке есть \"compare_ssim\", заменим на \"structural_similarity\"\n",
        "        if \"from skimage.measure import compare_ssim\" in line:\n",
        "            file.write(\"from skimage.metrics import structural_similarity\\n\")\n",
        "        else:\n",
        "            file.write(line)\n",
        "\n",
        "print(\"Замена выполнена успешно!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DPIfAAk-7E9-"
      },
      "outputs": [],
      "source": [
        "# Зададим девайс: cuda или cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "DAuBIaWQ7N6_",
        "outputId": "b64fe670-fee8-4e94-de15-d4117b9e5779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT\n",
            "From (redirected): https://drive.google.com/uc?id=1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT&confirm=t&uuid=fe6caed8-e102-4589-abc8-ebdcafb3811f\n",
            "To: /content/stylegan2-pytorch/stylegan2-ffhq-config-f.pt\n",
            "100%|██████████| 381M/381M [00:01<00:00, 242MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'stylegan2-ffhq-config-f.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# URL for the Google Drive file\n",
        "url = \"https://drive.google.com/uc?id=1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT\"\n",
        "\n",
        "# Download the file\n",
        "gdown.download(url, \"stylegan2-ffhq-config-f.pt\", quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DswhREmsUm2"
      },
      "source": [
        "### CLIP Loss\n",
        "\n",
        "We want the model to generate images that align with the properties described in the text prompt. To achieve this, we use CLIP loss to encourage the model to generate images that are closer to the text description in the CLIP embedding space.\n",
        "\n",
        "The CLIP loss is computed as follows:\n",
        "\n",
        "$$\n",
        "D_{\\text{CLIP}} = 1 - \\frac{\\langle E_T(G(w)) \\cdot E_T(t) \\rangle }{\\|E_T(G(w))\\| \\cdot \\|E_T(t)\\|}\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "\n",
        "- $E_T$: Text and image encoder function (e.g., CLIP's shared encoder for both text and image).\n",
        "- $t$: Text description (prompt).\n",
        "- $G(w)$: Image generated by the generator using the latent vector $w$.\n",
        "- $D_{\\text{CLIP}}$: Cosine distance between the text embedding and the image embedding in the CLIP space.\n",
        "\n",
        "**Intuition:** The CLIP loss encourages the generated image $G(w)$ to be semantically aligned with the provided text prompt $t$. If the image and text embeddings are close in the CLIP space, the loss will be small, helping the model generate images that reflect the described properties.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0sc72kwOkJRN"
      },
      "outputs": [],
      "source": [
        "class CLIPLoss(torch.nn.Module):\n",
        "    def __init__(self, stylegan_size=1024):\n",
        "        super(CLIPLoss, self).__init__()\n",
        "\n",
        "        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "        # clip only supports images 224*224.\n",
        "        # we need to resize image with min information loss possible. therefore we firstly apply upsamling, and then pooling.\n",
        "        # scale_factor=7 allows not to deal with decimals\n",
        "        self.upsample = torch.nn.Upsample(scale_factor=7)\n",
        "        self.avg_pool = torch.nn.AvgPool2d(kernel_size=(1024 * 7 // 224))\n",
        "\n",
        "    def forward(self, prompt, image):\n",
        "        # text preprocessing\n",
        "        tokenized_prompt = clip.tokenize([prompt]).to(device)\n",
        "        # image preprocessing\n",
        "        preprocessed_image = self.avg_pool(self.upsample(image))\n",
        "        # encoding to CLIP space\n",
        "        text_features = self.model.encode_text(tokenized_prompt)  # Remove .detach()\n",
        "        image_features = self.model.encode_image(preprocessed_image)  # Remove .detach()\n",
        "\n",
        "        # normalize embeddings\n",
        "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "        similarity = (image_features * text_features).sum(dim=1)\n",
        "        loss = 1 - similarity.mean()\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v-9XVx5ys_y"
      },
      "source": [
        "### CLIP Directional loss\n",
        "\n",
        "We want to only change properties described in our text prompt. We use CLIP loss to panish model for changing other properties.\n",
        "\n",
        "The directional loss is computed as follows:\n",
        "\n",
        "$$\n",
        "\\Delta T = E_T(t_{\\text{target}}) - E_T(t_{\\text{source}})\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Delta I = E_I(G_{\\text{train}}(w)) - E_I(G_{\\text{frozen}}(w))\n",
        "$$\n",
        "\n",
        "$$\n",
        "L_{\\text{direction}} = 1 - \\frac{\\Delta I \\cdot \\Delta T}{|\\Delta I| \\cdot |\\Delta T|}\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "\n",
        "- $E_T$: Text encoder function (e.g., CLIP text encoder).\n",
        "- $E_I$: Image encoder function (e.g., CLIP image encoder).\n",
        "- $t_{\\text{target}}$: Target text description.\n",
        "- $t_{\\text{source}}$: Source text description.\n",
        "- $G_{\\text{train}}$: Trainable generator (e.g., StyleGAN during training).\n",
        "- $G_{\\text{frozen}}$: Frozen generator (pre-trained StyleGAN, unmodified during training).\n",
        "- $w$: Latent vector input to the generator.\n",
        "\n",
        "**Intuition:** If the image changes $\\Delta I$ don’t align with the text changes $\\Delta T$, the loss increases, encouraging the generator to better follow the text guidance during optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wewklB2Lz6Cs"
      },
      "outputs": [],
      "source": [
        "class CLIPDirectionalLoss(torch.nn.Module):\n",
        "    def __init__(self, stylegan_size=1024):\n",
        "        super(CLIPDirectionalLoss, self).__init__()\n",
        "\n",
        "        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "        # clip only supports images 224*224.\n",
        "        # we need to resize image with min information loss possible. therefore we firstly apply upsamling, and then pooling.\n",
        "        # scale_factor=7 allows not to deal with decimals\n",
        "        self.upsample = torch.nn.Upsample(scale_factor=7)\n",
        "        self.avg_pool = torch.nn.AvgPool2d(kernel_size=(1024 * 7 // 224))\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        frozen_generator_image,\n",
        "        trainable_generator_image,\n",
        "        source_prompt,\n",
        "        target_prompt,\n",
        "    ):\n",
        "        # text preprocessing\n",
        "        tokenized_source_prompt = clip.tokenize([source_prompt]).to(device)\n",
        "        tokenized_target_prompt = clip.tokenize([target_prompt]).to(device)\n",
        "        # image preprocessing\n",
        "        preprocessed_frozen_generator_image = self.avg_pool(\n",
        "            self.upsample(frozen_generator_image)\n",
        "        )\n",
        "        preprocessed_trainable_generator_image = self.avg_pool(\n",
        "            self.upsample(trainable_generator_image)\n",
        "        )\n",
        "\n",
        "        # encoding to CLIP space\n",
        "        source_prompt_features = self.model.encode_text(tokenized_source_prompt)\n",
        "        target_prompt_features = self.model.encode_text(tokenized_target_prompt)\n",
        "        frozen_generator_image_features = self.model.encode_image(\n",
        "            preprocessed_frozen_generator_image\n",
        "        )\n",
        "        trainable_generator_image_features = self.model.encode_image(\n",
        "            preprocessed_trainable_generator_image\n",
        "        )\n",
        "\n",
        "        delta_t = target_prompt_features - source_prompt_features\n",
        "        delta_i = trainable_generator_image_features - frozen_generator_image_features\n",
        "\n",
        "        # normalize deltas\n",
        "        delta_i = delta_i / delta_i.norm(dim=1, keepdim=True)\n",
        "        delta_t = delta_t / delta_t.norm(dim=1, keepdim=True)\n",
        "\n",
        "        loss = 1 - (delta_i * delta_t).sum(dim=1).mean()\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uu5WIuIBBXAy",
        "outputId": "ece776a3-4b88-4cbe-ee17-5dab3bbadf35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:05<00:00, 64.0MiB/s]\n"
          ]
        }
      ],
      "source": [
        "clip_loss = CLIPLoss()\n",
        "clip_directional_loss = CLIPDirectionalLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "B5GJ3KnV7eY9"
      },
      "outputs": [],
      "source": [
        "# Параметры генератора\n",
        "size = 1024  # Размер изображения\n",
        "latent_dim = 512  # Размер латентного пространства\n",
        "n_mlp = 8  # Количество слоев MLP\n",
        "channel_multiplier = 2\n",
        "ckpt = \"/content/stylegan2-pytorch/stylegan2-ffhq-config-f.pt\"  # Путь к весам StyleGAN2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1N4GSV67fA7",
        "outputId": "9831be9a-bd7a-4a55-e86d-c3b889ce106f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-0f2224f1e908>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(ckpt)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "frozen_generator = Generator(\n",
        "    size, latent_dim, n_mlp, channel_multiplier=channel_multiplier\n",
        ").to(device)\n",
        "trainable_generator = Generator(\n",
        "    size, latent_dim, n_mlp, channel_multiplier=channel_multiplier\n",
        ").to(device)\n",
        "\n",
        "frozen_generator.eval()\n",
        "trainable_generator.train()\n",
        "\n",
        "checkpoint = torch.load(ckpt)\n",
        "\n",
        "frozen_generator.load_state_dict(checkpoint[\"g_ema\"])\n",
        "trainable_generator.load_state_dict(checkpoint[\"g_ema\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "omK3R3RY8zKP"
      },
      "outputs": [],
      "source": [
        "def freeze_stylegan_layers(generator, selected_layers=None):\n",
        "    \"\"\"\n",
        "    Freezes specific layers in a StyleGAN generator.\n",
        "\n",
        "    Args:\n",
        "        generator: The StyleGAN generator model.\n",
        "        selected_layers: A list of indices of layers that should remain trainable.\n",
        "                         If None, only Mapping Network, Affine Transformations, and ToRGB layers are frozen.\n",
        "    \"\"\"\n",
        "    # Always freeze Mapping Network (style), ToRGB layers, and Affine Transformations\n",
        "    modules_to_freeze = [\"style\", \"to_rgb1\"]\n",
        "\n",
        "    for module in modules_to_freeze:\n",
        "        if hasattr(generator, module):\n",
        "            for param in getattr(generator, module).parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # Freeze all ToRGB layers in the ModuleList\n",
        "    if hasattr(generator, \"to_rgbs\"):\n",
        "        for to_rgb_layer in generator.to_rgbs:\n",
        "            for param in to_rgb_layer.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # If selected_layers is provided, freeze all other conv layers\n",
        "    if selected_layers is not None:\n",
        "        for i, conv in enumerate(generator.convs):\n",
        "            if i not in selected_layers:  # Freeze layers not in the selected list\n",
        "                for param in conv.parameters():\n",
        "                    param.requires_grad = False\n",
        "            else:\n",
        "                for param in conv.parameters():\n",
        "                    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZdDs79pY9L28"
      },
      "outputs": [],
      "source": [
        "freeze_stylegan_layers(frozen_generator)\n",
        "freeze_stylegan_layers(trainable_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jv6IiPNMu4Gy"
      },
      "outputs": [],
      "source": [
        "batch_size = 4  # Number of images to use in each step\n",
        "num_steps = 61\n",
        "\n",
        "l2_lambda = 1.0\n",
        "clip_lambda = 2.5\n",
        "clip_directional_lambda = 1.2\n",
        "id_lambda = 0.0005\n",
        "num_layers = 18\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(trainable_generator.parameters(), lr=0.01)\n",
        "\n",
        "source_prompt = \"photo\"\n",
        "target_prompt = \"zombie\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc"
      ],
      "metadata": {
        "id": "p4rYpppgvtAk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HpuBXt3XvDa_"
      },
      "outputs": [],
      "source": [
        "def get_k_relevant_layers(generator, k, target_prompt):\n",
        "    latent_z = torch.randn((batch_size, latent_dim), device=device)\n",
        "\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        latent_w = generator.style(latent_z).detach()\n",
        "        latent_w_plus = latent_w.unsqueeze(1).repeat(1, num_layers, 1).detach().clone()\n",
        "\n",
        "    latent_w_plus.requires_grad_(True)  # Указываем requires_grad_ отдельно\n",
        "\n",
        "    # Сохраняем начальное состояние\n",
        "    initial_w_plus = latent_w_plus.clone().detach()\n",
        "\n",
        "    relevant_optimizer = torch.optim.Adam([latent_w_plus], lr=0.01)\n",
        "\n",
        "    for step in range(10):\n",
        "        relevant_optimizer.zero_grad()\n",
        "\n",
        "        image, _ = generator(\n",
        "            [latent_w_plus], input_is_latent=True, randomize_noise=False\n",
        "        )\n",
        "        loss = clip_lambda * clip_loss(target_prompt, image)\n",
        "\n",
        "        loss.backward()  # `loss.requires_grad = True` не нужен\n",
        "        relevant_optimizer.step()\n",
        "\n",
        "        del image, _, loss\n",
        "\n",
        "    # Вычисляем изменения после оптимизации\n",
        "    delta_w = (latent_w_plus.detach() - initial_w_plus).abs().mean(dim=(0, 2))\n",
        "\n",
        "    # Выбираем k слоёв с наибольшими изменениями\n",
        "    relevant_layers = [i - 1 for i in delta_w.topk(k).indices.tolist()]\n",
        "    print(\"relevant_layers\", relevant_layers)\n",
        "\n",
        "    gc.collect()  # Принудительно запускаем garbage collector\n",
        "    torch.cuda.empty_cache()  # Очищаем кеш GPU\n",
        "\n",
        "    return relevant_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSPVmts1G_hV"
      },
      "outputs": [],
      "source": [
        "losses = {\"l2\": [], \"clip\": [], \"clip_directional\": [], \"total\": []}\n",
        "\n",
        "# Количество стилевых модулей в генераторе (обычно 18 для StyleGAN2)\n",
        "for step in range(num_steps):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if step % 20 == 0:\n",
        "        k_relevant_layers = get_k_relevant_layers(trainable_generator, 5, target_prompt)\n",
        "        freeze_stylegan_layers(trainable_generator, k_relevant_layers)\n",
        "\n",
        "    # Генерация латентных векторов\n",
        "    latent_z = torch.randn((batch_size, latent_dim), device=device)\n",
        "\n",
        "    # Проекция в W+ пространство\n",
        "    latent_w = frozen_generator.style(latent_z).detach().clone()\n",
        "    latent_w_plus = latent_w.unsqueeze(1).repeat(\n",
        "        1, num_layers, 1\n",
        "    )  # [batch_size, num_layers, latent_dim]\n",
        "\n",
        "    # Генерация изображений в W+ пространстве\n",
        "    with torch.no_grad():\n",
        "        frozen_generator_images, _ = frozen_generator(\n",
        "            [latent_w_plus], input_is_latent=True, randomize_noise=False\n",
        "        )\n",
        "    trainable_generator_images, _ = trainable_generator(\n",
        "        [latent_w_plus], input_is_latent=True, randomize_noise=False\n",
        "    )\n",
        "\n",
        "    # Вычисление потерь\n",
        "    l2_loss_value = torch.mean(\n",
        "        (frozen_generator_images - trainable_generator_images) ** 2\n",
        "    )\n",
        "    clip_loss_value = clip_loss(target_prompt, trainable_generator_images)\n",
        "    clip_directional_loss_value = clip_directional_loss(\n",
        "        frozen_generator_images,\n",
        "        trainable_generator_images,\n",
        "        source_prompt,\n",
        "        target_prompt,\n",
        "    )\n",
        "\n",
        "    # Комбинирование потерь\n",
        "    loss = (\n",
        "        l2_lambda * l2_loss_value\n",
        "        + clip_lambda * clip_loss_value\n",
        "        + clip_directional_lambda * clip_directional_loss_value\n",
        "    )\n",
        "\n",
        "    # Логирование потерь\n",
        "    losses[\"l2\"].append(l2_loss_value.detach().item())\n",
        "    losses[\"clip\"].append(clip_loss_value.detach().item())\n",
        "    losses[\"clip_directional\"].append(clip_directional_loss_value.detach().item())\n",
        "    losses[\"total\"].append(loss.detach().item())\n",
        "\n",
        "    # Обратное распространение ошибки и обновление весов\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Visualization and logging\n",
        "    if step % 20 == 0 or step == num_steps - 1:\n",
        "        print(f\"Step [{step}/{num_steps}], Loss: {losses['total'][-1]}\")\n",
        "\n",
        "        # Create subplots to visualize both frozen and trainable images side-by-side\n",
        "        fig, axs = plt.subplots(\n",
        "            2, batch_size, figsize=(batch_size * 4, 8)\n",
        "        )  # 2 rows, batch_size columns\n",
        "\n",
        "        # If batch_size = 1, axs will not be iterable, so wrap it in a list\n",
        "        if batch_size == 1:\n",
        "            axs = [[axs[0]], [axs[1]]]\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Display frozen_generator_images (row 0)\n",
        "            axs[0][i].imshow(\n",
        "                (\n",
        "                    frozen_generator_images.cpu()\n",
        "                    .detach()[i]\n",
        "                    .clip(-1, 1)\n",
        "                    .permute(1, 2, 0)\n",
        "                    + 1\n",
        "                )\n",
        "                / 2\n",
        "            )\n",
        "            axs[0][i].set_title(f\"Frozen Image {i+1}\")\n",
        "            axs[0][i].axis(\"off\")\n",
        "\n",
        "            # Display trainable_generator_images (row 1)\n",
        "            axs[1][i].imshow(\n",
        "                (\n",
        "                    trainable_generator_images.cpu()\n",
        "                    .detach()[i]\n",
        "                    .clip(-1, 1)\n",
        "                    .permute(1, 2, 0)\n",
        "                    + 1\n",
        "                )\n",
        "                / 2\n",
        "            )\n",
        "            axs[1][i].set_title(f\"Trainable Image {i+1}\")\n",
        "            axs[1][i].axis(\"off\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Clear cached data to free up memory\n",
        "    del frozen_generator_images, trainable_generator_images, latent_w, latent_z\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_OBEj4sPjs3"
      },
      "outputs": [],
      "source": [
        "# Steps corresponding to the losses\n",
        "steps = range(len(losses[\"total\"]))\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "fig.suptitle(\"Losses Over Steps\", fontsize=16)\n",
        "\n",
        "# Titles and keys for the losses\n",
        "titles = [\"L2 Loss\", \"CLIP Loss\", \"CLIP Directional\", \"Total Loss\"]\n",
        "loss_keys = [\"l2\", \"clip\", \"clip_directional\", \"total\"]\n",
        "\n",
        "# Plot each loss\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    loss_key = loss_keys[i]\n",
        "    ax.plot(steps, losses[loss_key], marker=\"o\", label=f\"{loss_key.upper()} Loss\")\n",
        "    ax.set_title(titles[i])\n",
        "    ax.set_xlabel(\"Step\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "\n",
        "# Adjust layout to fit the title and spacing\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRbzBgChiF3m"
      },
      "source": [
        "Inversion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bR0-zhniaqP"
      },
      "outputs": [],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC-KJJ54ibz5"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/omertov/encoder4editing.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IOx7qAJidX-"
      },
      "outputs": [],
      "source": [
        "%cd encoder4editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7c3M9YCifTy"
      },
      "outputs": [],
      "source": [
        "# Импортируем необходимые библиотеки\n",
        "from models.psp import pSp  # Импортируем модель pSp\n",
        "from argparse import Namespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sEppenjh0gR"
      },
      "outputs": [],
      "source": [
        "# URL for the Google Drive file\n",
        "url = \"https://drive.google.com/uc?id=1cUv_reLE6k3604or78EranS7XzuVMWeO\"\n",
        "\n",
        "# Download the file\n",
        "gdown.download(url, \"e4e_ffhq_encode.pt\", quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EELNUC6QpGdB"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzGKu0AciSMA"
      },
      "outputs": [],
      "source": [
        "# Определяем преобразования для изображения\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((256, 256)),  # Изменяем размер изображения до 256x256\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
        "        ),  # Нормализуем значения пикселей\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uqUFYaIijGl"
      },
      "outputs": [],
      "source": [
        "# Загружаем предобученную модель pSp\n",
        "model_path = \"/content/encoder4editing/e4e_ffhq_encode.pt\"  # Путь к весам энкодера\n",
        "ckpt = torch.load(model_path, map_location=\"cpu\")\n",
        "opts = ckpt[\"opts\"]\n",
        "\n",
        "opts[\"checkpoint_path\"] = model_path\n",
        "opts = Namespace(**opts)\n",
        "net = pSp(opts)\n",
        "net.eval()\n",
        "net.to(device)\n",
        "print(\"Model successfully loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Avz-e8xMi8VI"
      },
      "outputs": [],
      "source": [
        "# Загружаем и отображаем изображение\n",
        "from PIL import Image\n",
        "\n",
        "image_path = \"/content/images/mikita.jpeg\"  # Путь к изображению\n",
        "original_image = Image.open(image_path)\n",
        "original_image = original_image.convert(\"RGB\")  # Преобразуем изображение в RGB формат\n",
        "original_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTw3QpE9puzf"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of10C4bki-Sq"
      },
      "outputs": [],
      "source": [
        "# Определяем тип эксперимента\n",
        "experiment_type = \"ffhq_encode\"\n",
        "\n",
        "# Загружаем модель для определения ключевых точек лица - понадобится нам для выравнивания\n",
        "if experiment_type == \"ffhq_encode\" and 'shape_predictor_68_face_landmarks.dat' not in os.listdir():\n",
        "    !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2  # Скачиваем архив с моделью\n",
        "    !bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2  # Распаковываем архив\n",
        "\n",
        "# Задаем размер изображения для изменения\n",
        "resize_dims = (256, 256)\n",
        "\n",
        "\n",
        "def run_alignment(image_path):\n",
        "  \"\"\"Выравнивает лицо на изображении, используя dlib.\n",
        "\n",
        "  Args:\n",
        "      image_path: Путь к файлу изображения.\n",
        "\n",
        "  Returns:\n",
        "      Выровненное изображение в формате PIL.Image.\n",
        "  \"\"\"\n",
        "  import dlib\n",
        "  from utils.alignment import align_face  # Функция для выравнивания лица\n",
        "  predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "  aligned_image = align_face(filepath=image_path, predictor=predictor)\n",
        "  print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
        "  return aligned_image  # Возвращаем выровненное изображение\n",
        "\n",
        "# Выравниваем лицо, если тип эксперимента - кодирование FFHQ\n",
        "if experiment_type == \"ffhq_encode\":\n",
        "  input_image = run_alignment(image_path)\n",
        "else:\n",
        "  input_image = original_image\n",
        "\n",
        "\n",
        "input_image.resize(resize_dims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyXCK_DBlDgf"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l54jm8ilDKP"
      },
      "outputs": [],
      "source": [
        "# Посмотрим на конечный результат\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axs[0].imshow(np.asarray(input_image))\n",
        "axs[0].set_title(\"Исходное изображение\")\n",
        "axs[0].axis(\"off\")\n",
        "\n",
        "axs[1].imshow(np.asarray(original_image))\n",
        "axs[1].set_title(\"Выровненное изображение\")\n",
        "axs[1].axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxoc9UVSlQAk"
      },
      "outputs": [],
      "source": [
        "transformed_image = transform(\n",
        "    input_image\n",
        ")  # Применяем преобразования, определенные ранее\n",
        "\n",
        "\n",
        "# Определяем функцию для отображения результата рядом с исходным\n",
        "def display_alongside_source_image(result_image, source_image):\n",
        "    \"\"\"Отображает результат рядом с исходным изображением.\n",
        "\n",
        "    Args:\n",
        "        result_image: Результирующее изображение (PIL.Image).\n",
        "        source_image: Исходное изображение (PIL.Image).\n",
        "\n",
        "    Returns:\n",
        "        Объединенное изображение (PIL.Image).\n",
        "    \"\"\"\n",
        "    res = np.concatenate(\n",
        "        [\n",
        "            np.array(source_image.resize(resize_dims)),\n",
        "            np.array(result_image.resize(resize_dims)),\n",
        "        ],\n",
        "        axis=1,\n",
        "    )\n",
        "    return Image.fromarray(res)\n",
        "\n",
        "\n",
        "def run_on_batch(inputs, net):\n",
        "    \"\"\"Запускает модель на пакете данных.\n",
        "\n",
        "    Args:\n",
        "        inputs: Входные данные (тензор PyTorch).\n",
        "        net: Модель pSp.\n",
        "\n",
        "    Returns:\n",
        "        Сгенерированные изображения и латентные векторы.\n",
        "    \"\"\"\n",
        "    images, latents = net(\n",
        "        inputs.to(\"cuda\").float(), randomize_noise=False, return_latents=True\n",
        "    )  # Запускаем модель\n",
        "    return images, latents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5Yx8YBdlS8Q"
      },
      "outputs": [],
      "source": [
        "# Запускаем модель и измеряем время выполнения\n",
        "import time  # Модуль для работы со временем\n",
        "from utils.common import tensor2im\n",
        "import numpy as np\n",
        "\n",
        "with torch.no_grad():\n",
        "    tic = time.time()  # Время начала выполнения\n",
        "    images, latents = run_on_batch(transformed_image.unsqueeze(0), net)\n",
        "    result_image, latent = (\n",
        "        images[0],\n",
        "        latents[0],\n",
        "    )  # Извлекаем результат и латентный вектор\n",
        "    toc = time.time()  # Время окончания выполнения\n",
        "    print(\"Inference took {:.4f} seconds.\".format(toc - tic))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puJjd2XFqP4g"
      },
      "outputs": [],
      "source": [
        "latent.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KhM1M4aqKxC"
      },
      "outputs": [],
      "source": [
        "# Отображаем результат инверсии\n",
        "display_alongside_source_image(tensor2im(result_image), input_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO_ntbUcquai"
      },
      "outputs": [],
      "source": [
        "trainable_generator.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d7VmfPaqso-"
      },
      "outputs": [],
      "source": [
        "trainable_generator_images, _ = trainable_generator(\n",
        "    [latent], input_is_latent=True, randomize_noise=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFWcjAyNrAV2"
      },
      "outputs": [],
      "source": [
        "del frozen_generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDG-Dx-jrN6o"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laGu5VUPih3C"
      },
      "source": [
        "Cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijUskZ8ZWUk8"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# # Move tensors to CPU and delete them\n",
        "# del frozen_generator  # Assuming `model` is your trained model\n",
        "# del trainable_generator  # Assuming `model` is your trained model\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# # Collect garbage to clear references\n",
        "# gc.collect()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}